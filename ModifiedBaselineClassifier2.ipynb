{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mary/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mary/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mary/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /Users/mary/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "#testing out git version control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve,f1_score,auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mary/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set seed for reproduceable results\n",
    "np.random.seed(500)\n",
    "# Read in to Pandas DataFrame and drop the first row(which contained column names as I have assigned new names)\n",
    "reviews = pd.read_csv(r\"rt_reviews.csv\", names = ['target', 'review'], encoding = 'latin-1')\n",
    "reviews = reviews.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize target data types to numeric\n",
    "for i in range(reviews.shape[0]):\n",
    "    if(reviews['target'].values[i] == '0'):\n",
    "        reviews['target'].values[i] = 0\n",
    "    elif(reviews['target'].values[i] == '1'):\n",
    "        reviews['target'].values[i] = 1\n",
    "for i in range(reviews.shape[0]):\n",
    "    if(isinstance(reviews['target'].values[i],str)):\n",
    "        print(\"Caught: \",reviews['target'].values[i])\n",
    "    if(reviews['target'].values[i] != 0 and reviews['target'].values[i] != 1):\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose number of samples to use\n",
    "reviews = reviews.iloc[:2000] #commented out to use all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove and row where either datafield is blank, no rows contained blank data so the shape remains 480000,2 \n",
    "reviews.dropna(inplace = True)\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text to lowercase\n",
    "reviews['review'] = [entry.lower() for entry in reviews['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each review: this process converts each review into a set of words. \n",
    "reviews['review'] = [word_tokenize(entry) for entry in reviews['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Lemmatizing ##############  ##DONT RUN W/ ALL SAMPLES\n",
    "\n",
    "# Creating tags so that lemmatizer can understand verbs from nouns from adjectives \n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "for index,entry in enumerate(reviews['review']):\n",
    "    index = index+1 # Index seems to off by one, this fixes it\n",
    "    # Words that follow the rules will end up in this list\n",
    "    Final_words = []\n",
    "    \n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    \n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha() and wordnet.synsets(word):\n",
    "            word_Final = word_Lemmatized.lemmatize(word, tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "        reviews.loc[index, 'lemmatized_words'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Unaltered Words ############\n",
    "\n",
    "for index, entry in enumerate(reviews['review']):\n",
    "    index = index + 1\n",
    "    Final_words = []\n",
    "        \n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha() and wordnet.synsets(word):\n",
    "            word_Final = word  #This is where a stemmer would go\n",
    "            Final_words.append(word_Final)\n",
    "        reviews.loc[index, 'unaltered_words'] = str(Final_words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Stemming Words ###############  ##DONT RUN W/ ALL SAMPLES\n",
    "\n",
    "for index, entry in enumerate(reviews['review']):\n",
    "    index = index + 1\n",
    "    Final_words = []\n",
    "        \n",
    "    word_Stemmer = PorterStemmer()\n",
    "        \n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha() and wordnet.synsets(word):\n",
    "            word_Final = word_Stemmer.stem(word)  #This is where a stemmer would go\n",
    "            Final_words.append(word_Final)\n",
    "        reviews.loc[index, 'stemmed_words'] = str(Final_words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train and Test splits \n",
    "test_s = .15\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(reviews['unaltered_words'], \n",
    "                                                                    reviews['target'], test_size=test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\ntrain_x Shape: \", train_x.shape)\n",
    "print(\"\\ntrain_y Shape: \", train_y.shape)\n",
    "print(\"\\ntest_x Shape: \", test_x.shape)\n",
    "print(\"\\ntest_y Shape: \", test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target data\n",
    "Encoder = LabelEncoder()\n",
    "train_y = Encoder.fit_transform(train_y)\n",
    "test_y = Encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Term Frequency-Inverse Document Frequency #######\n",
    "max_f = 100000\n",
    "Tfidf_vect = TfidfVectorizer(stop_words='english', max_features=max_f)\n",
    "Tfidf_vect.fit_transform(reviews['unaltered_words'])\n",
    "train_x_Tfidf = Tfidf_vect.transform(train_x)\n",
    "test_x_Tfidf = Tfidf_vect.transform(test_x)\n",
    "#print(Tfidf_vect.vocabulary_)\n",
    "len(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Bag of Words #######\n",
    "max_f = 100000\n",
    "#ngram_range default is (1,1)\n",
    "#ngram_range(1,1) -> uni-gram\n",
    "#ngram_range(2,2) -> bi-gram\n",
    "#ngram_range(3,3) -> tri-gram\n",
    "#ngram_range(1,2) -> uni, bi-grams -> BEST SO FAR\n",
    "Count_vect = CountVectorizer(stop_words='english', max_features=max_f, ngram_range=(1,2))\n",
    "Count_vect.fit_transform(reviews['unaltered_words'])\n",
    "train_x_Count = Count_vect.transform(train_x)\n",
    "test_x_Count = Count_vect.transform(test_x)\n",
    "#print(Tfidf_vect.vocabulary_)\n",
    "len(Count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python dictionary of various test results: \n",
    "\n",
    "# These test results use a data set that contains no english words, is lemmatized\n",
    "\n",
    "zach_test_results = {\n",
    "    # Max features percentages are out of 36131(which is max features when\n",
    "    # full 480000 sampels and non english words removed, .15 test size)\n",
    "    \n",
    "    # 9033 features \n",
    "    \"max-features_.25\" : {\"accuracy\" : 0.7748 , \"f1-avg\" : 0.775 },\n",
    "    \n",
    "    # 18066\n",
    "    \"max-features_.50\" : {\"accuracy\" : 0.7810 , \"f1-avg\" : 0.78 },\n",
    "    \n",
    "    # 27120\n",
    "    \"max-features_.75\" : {\"accuracy\" : 0.7841 , \"f1-avg\" : 0.78 },\n",
    "    \n",
    "    #30711\n",
    "    \"max-features_.85\" : {\"accuracy\" : 0.7845, \"f1-avg\" : 0.785 },\n",
    "    \n",
    "    # 34324\n",
    "    \"max-features_.95\" : {\"accuracy\" : 0.7845, \"f1-avg\" : 0.785},\n",
    "    \n",
    "    # 36131\n",
    "    \"max-features_1.00\" : {\"accuracy\" : 0.7845, \"f1-avg\" : 0.785},\n",
    "    \n",
    "    \n",
    "    # Removing words with low information \n",
    "    \n",
    "    # results in only 6027 features meaning the majority are below 100 df\n",
    "    \"min_df_100\" : {\"accuracy\" : 0.7673 , \"f1-avg\": 0.77 },\n",
    "    \n",
    "    # results in 9060 feautures\n",
    "    \"min_df_50\" : {\"accuracy\" : 0.7748 , \"f1-avg\": 0.775 },\n",
    "    \n",
    "    # results in 18458 features\n",
    "    \"min_df_10\" : {\"accuracy\" : 0.7811 , \"f1-avg\": 0.78},\n",
    "    \n",
    "    # results in 23402 features\n",
    "    \"min_df_5\" : {\"accuracy\" : 0.7833, \"f1-avg\": 0.78 },\n",
    "    \n",
    "    #results in 36131 features\n",
    "    \"min_df_1\" : {\"accuracy\" : 0.7845, \"f1-avg\": 0.785 },\n",
    "    \n",
    "    #results in 36131 features\n",
    "    \"min_df_0\" : {\"accuracy\" : 0.7845, \"f1-avg\": 0.785}, \n",
    "    \n",
    "    # when non english features taken out only 36131 features\n",
    "    # when non english words not taken out 69635\n",
    "    \"max-samples-norestriction-wo-nonenglish-test.15\": {\"accuracy\" : 0.7845, \"f1-avg\" : 0.785},\n",
    "    \"max-samples-norestriction-w-nonenglish-test.15\": {\"accuracy\" : 0.79275, \"f1-avg\" : 0.79},\n",
    "    \n",
    "    # trained with 480K samples and titles removed\n",
    "    #36131 features\n",
    "    \"lemma\" : {\"accuracy\" : .7833 , 'f1-avg' : .78},\n",
    "    #24835 features \n",
    "    \"stem\" : {\"accuracy\" : .7792 , 'f1-avg' : .78},\n",
    "    #47506 features \n",
    "    \"unaltered\" : {\"accuracy\" : .7945, 'f1-avg' : .795}, \n",
    "    \n",
    "    # trained with 10K samples and titles removed\n",
    "    #12775 features\n",
    "    \"lemma\" : {\"accuracy\" : .708 , 'f1-avg' : .705},\n",
    "    #10402 features \n",
    "    \"stem\" : {\"accuracy\" : .7153 , 'f1-avg' : .715},\n",
    "    #15973 f \n",
    "    \"unaltered\" : {\"accuracy\" : .7093, 'f1-avg' : .71}, \n",
    "    \n",
    "     # trained with 100K samples and titles removed\n",
    "    #26600 features\n",
    "    \"lemma\" : {\"accuracy\" : .77507 , 'f1-avg' : .775},\n",
    "    #19459 features \n",
    "    \"stem\" : {\"accuracy\" : .762 , 'f1-avg' : .76},\n",
    "    #34772 f \n",
    "    \"unaltered\" : {\"accuracy\" : .7752, 'f1-avg' : .78}, \n",
    "    \n",
    "    # best performance so far, but test split may be too small. \n",
    "    \"unaltered_w/_.05test\" : {\"accuracy\" : .796, 'f1-avg' : .8}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Term Frequency-Inverse Document Frequency #######\n",
    "nb = naive_bayes.MultinomialNB()\n",
    "nb.fit(train_x_Tfidf, train_y)\n",
    "\n",
    "\n",
    "# predict the target on validation data\n",
    "pred_nb = nb.predict(test_x_Tfidf)\n",
    "\n",
    "# output accuracy just to show it works\n",
    "print(\"Accuracy: \", accuracy_score(pred_nb, test_y))\n",
    "nb_probs = nb.predict_proba(test_x_Tfidf)\n",
    "nb_probs = nb_probs[:,1]\n",
    "print(classification_report(test_y, pred_nb, labels=[0,1]))\n",
    "# svm = SVC(probability=True)\n",
    "# svm.fit(train_x_Tfidf, train_y)\n",
    "# pred_svm = svm.predict(test_x_Tfidf)\n",
    "# print(\"Accuracy: \", accuracy_score(pred_svm, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python dictionary of various test results: \n",
    "\n",
    "\n",
    "mary_test_results = {\n",
    "    # full 480000 sampels and non english words removed, .15 test size)\n",
    "    \n",
    "    # (1,1)->only unigrams\n",
    "    \"only uni-grams\" : {\"accuracy\" : 0.7871 , \"f1-avg\" : 0.79 },\n",
    "    \n",
    "    # (2,2)-> only bigrams\n",
    "    \"only bi-grams\" : {\"accuracy\" : 0.7294 , \"f1-avg\" : 0.73 },\n",
    "    \n",
    "    # (3,3) -> only trigrams\n",
    "    \"only tri-grams\" : {\"accuracy\" : 0.5991 , \"f1-avg\" : 0.54 },\n",
    "    \n",
    "    #(4,4) -> only quad-grams\n",
    "    \"only quad-grams\" : {\"accuracy\" : 0.5746, \"f1-avg\" : 0.48 },\n",
    "    \n",
    "    # (1,2) -> uni and bi-grams -> BEST SO FAR\n",
    "    \"uni-grams, bi-grams\" : {\"accuracy\" : 0.8055, \"f1-avg\" : 0.81},\n",
    "    \n",
    "    # (1,3) -> uni, bi, and tri-grams\n",
    "    \"uni-grams, bi-grams, tri-grams\" : {\"accuracy\" : 0.8047, \"f1-avg\" : 0.80},\n",
    "    \n",
    "    # (1,4) -> uni, bi, tri, and quad-grams\n",
    "    \"uni-grams, bi-grams, tri-grams, quad-grams\" : {\"accuracy\" : 0.8049, \"f1-avg\" : 0.80},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Bag of Words Frequency #######\n",
    "nb = naive_bayes.MultinomialNB()\n",
    "nb.fit(train_x_Count, train_y)\n",
    "\n",
    "print(train_x_Count[0])\n",
    "\n",
    "# predict the target on validation data\n",
    "pred_nb = nb.predict(test_x_Count)\n",
    "\n",
    "# output accuracy just to show it works\n",
    "print(\"Accuracy: \", accuracy_score(pred_nb, test_y))\n",
    "nb_probs = nb.predict_proba(test_x_Count)\n",
    "nb_probs = nb_probs[:,1]\n",
    "print(classification_report(test_y, pred_nb, labels=[0,1]))\n",
    "# svm = SVC(probability=True)\n",
    "# svm.fit(train_x_Count, train_y)\n",
    "# pred_svm = svm.predict(test_x_Count)\n",
    "# print(\"Accuracy: \", accuracy_score(pred_svm, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Graphs of the Different grams for Bag of Words ############\n",
    "\n",
    "#performed on all 100,000 examples w/ unaltered words\n",
    "\n",
    "## only singular grams\n",
    "width = .3\n",
    "pltData = pd.DataFrame({ \n",
    "'accuracy': [.7871, .7294, .5991, .5746], \n",
    "'f1-score' : [.79,.73,.54,.48]})\n",
    "\n",
    "pltData[['f1-score', 'accuracy']].plot(kind = 'bar', width = width)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.xlim([-width, len(pltData['accuracy'])-width])\n",
    "\n",
    "ax.set_ylim([.45, .8])\n",
    "ax.set_xticklabels(('Uni', 'Bi', 'Tri','Quad'))\n",
    "ax.legend(labels = ['f1-score', 'accuracy'])\n",
    "ax.set_title(\"Performance by Singular Gram Type\")\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_xlabel('Gram type')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#only ranged grams\n",
    "width = .3\n",
    "pltData = pd.DataFrame({ \n",
    "'accuracy': [.7871, .8055, .8047, .8049], \n",
    "'f1-score' : [.79,.81,.80,.80]})\n",
    "\n",
    "pltData[['f1-score', 'accuracy']].plot(kind = 'bar', width = width)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.xlim([-width, len(pltData['accuracy'])-width])\n",
    "\n",
    "ax.set_ylim([.78, .82])\n",
    "ax.set_xticklabels(('Uni', 'Uni & Bi', 'Uni, Bi & Tri','Uni, Bi, Tri, &Quad'))\n",
    "ax.legend(labels = ['f1-score', 'accuracy'])\n",
    "ax.set_title(\"Performance by Ranged Gram Type\")\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_xlabel('Gram type')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#looking at uni-bi grams\n",
    "width = .3\n",
    "pltData = pd.DataFrame({ \n",
    "'accuracy': [.7871, .7294, .8055], \n",
    "'f1-score' : [.79, .73, .81]})\n",
    "\n",
    "pltData[['f1-score', 'accuracy']].plot(kind = 'bar', width = width)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.xlim([-width, len(pltData['accuracy'])-width])\n",
    "\n",
    "ax.set_ylim([.72, .82])\n",
    "ax.set_xticklabels(('Uni', 'Bi', 'Uni & Bi'))\n",
    "ax.legend(labels = ['f1-score', 'accuracy'])\n",
    "ax.set_title(\"Performance by Gram Type\")\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_xlabel('Gram type')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#looking at uni-tri grams\n",
    "width = .3\n",
    "pltData = pd.DataFrame({ \n",
    "'accuracy': [.7871, .7294, .5991, .8047], \n",
    "'f1-score' : [.79, .73, .54, .80]})\n",
    "\n",
    "pltData[['f1-score', 'accuracy']].plot(kind = 'bar', width = width)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.xlim([-width, len(pltData['accuracy'])-width])\n",
    "\n",
    "ax.set_ylim([.52, .82])\n",
    "ax.set_xticklabels(('Uni', 'Bi', 'Tri', 'Uni, Bi & Tri'))\n",
    "ax.legend(labels = ['f1-score', 'accuracy'])\n",
    "ax.set_title(\"Performance by Gram Type\")\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_xlabel('Gram type')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#looking at all grams\n",
    "width = .3\n",
    "pltData = pd.DataFrame({ \n",
    "'accuracy': [.7871, .7294, .5991, .5746, .8055, .8047, .8049], \n",
    "'f1-score' : [.79, .73, .54, .48, .81, .80, .80]})\n",
    "\n",
    "pltData[['f1-score', 'accuracy']].plot(kind = 'bar', width = width)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.xlim([-width, len(pltData['accuracy'])-width])\n",
    "\n",
    "ax.set_ylim([.45, .85])\n",
    "ax.set_xticklabels(('Uni', 'Bi', 'Tri', 'Quad', 'Uni & Bi', 'Uni, Bi & Tri', 'Uni, Bi, Tri, &Quad'))\n",
    "ax.legend(labels = ['f1-score', 'accuracy'])\n",
    "ax.set_title(\"Performance by Gram Type\")\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_xlabel('Gram type')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############### Lemma vs Stem vs Unaltered 480k ###################\n",
    "\n",
    "# performed on 480k samples with non english words removed\n",
    "width = .3\n",
    "pltData = pd.DataFrame({ \n",
    "'accuracy': [.7833, .7792, .7945], \n",
    "'f1-score' : [.78, .78,.795]})\n",
    "\n",
    "pltData[['f1-score', 'accuracy']].plot(kind = 'bar', width = width)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.xlim([-width, len(pltData['accuracy'])-width])\n",
    "\n",
    "ax.set_ylim([.77, .8])\n",
    "ax.set_xticklabels(('Lemmatized', 'Stemmed', 'Unaltered'))\n",
    "ax.legend(labels = ['f1-score', 'accuracy'])\n",
    "ax.set_title(\"Performance by Word Alteration type\")\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_xlabel('Word Alteration type')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "############### Lemma vs Stem vs Unaltered 10k ###################\n",
    "\n",
    "width = .3\n",
    "pltData = pd.DataFrame({ \n",
    "'accuracy': [.708, .7153, .7093], \n",
    "'f1-score' : [.705, .715,.71]})\n",
    "\n",
    "pltData[['f1-score', 'accuracy']].plot(kind = 'bar', width = width)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.xlim([-width, len(pltData['accuracy'])-width])\n",
    "\n",
    "ax.set_ylim([.7, .72])\n",
    "ax.set_xticklabels(('Lemmatized', 'Stemmed', 'Unaltered'))\n",
    "ax.legend(labels = ['f1-score', 'accuracy'])\n",
    "ax.set_title(\"Performance by Word Alteration type\")\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_xlabel('Word Alteration type')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "############### Lemma vs Stem vs Unaltered 100k ###################\n",
    "\n",
    "width = .3\n",
    "pltData = pd.DataFrame({ \n",
    "'accuracy': [.7751, .762, .7752], \n",
    "'f1-score' : [.775, .76,.78]})\n",
    "\n",
    "pltData[['f1-score', 'accuracy']].plot(kind = 'bar', width = width)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.xlim([-width, len(pltData['accuracy'])-width])\n",
    "\n",
    "ax.set_ylim([.75, .785])\n",
    "ax.set_xticklabels(('Lemmatized', 'Stemmed', 'Unaltered'))\n",
    "ax.legend(labels = ['f1-score', 'accuracy'])\n",
    "ax.set_title(\"Performance by Word Alteration type\")\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_xlabel('Word Alteration type')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############### Alternative Plot for minumum document frequency ###################\n",
    "\n",
    "width = .25 \n",
    "pltData = pd.DataFrame({ \n",
    "'accuracy': [.7845, .7833, .7811, .7748, .7673], \n",
    "'f1-score' : [.785, .78, .78, .775, .77], \n",
    "'avg' : [.78475, .7817, .7805, .7749, .7686]})\n",
    "\n",
    "pltData[['f1-score', 'accuracy']].plot(kind = 'bar', width = width)\n",
    "pltData['avg'].plot(color = \"tab:cyan\")\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.xlim([-width, len(pltData['accuracy'])-width])\n",
    "\n",
    "ax.set_ylim([.76, .79])\n",
    "ax.set_xticklabels(('1', '5', '10', '50', '100'))\n",
    "ax.legend(labels = ['avg','f1-score', 'accuracy'])\n",
    "ax.set_title(\"Performance by Minimum document frequency\")\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_xlabel('Minimum Document Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Plot for Max features ###################\n",
    "\n",
    "width = .25 \n",
    "pltData = pd.DataFrame({ \n",
    "'accuracy': [.7748, .7810, .7841, .7845, .7845, .7845], \n",
    "'f1-score' : [.775, .78, .78, .785, .785, .785], \n",
    "'avg' : [.7749, .7805, .7820, .78475, .78475, .78475]})\n",
    "\n",
    "pltData[['f1-score', 'accuracy']].plot(kind = 'bar', width = width)\n",
    "pltData['avg'].plot(color = \"tab:cyan\")\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.xlim([-width, len(pltData['accuracy'])-width])\n",
    "\n",
    "ax.set_ylim([.77, .79])\n",
    "ax.set_xticklabels(('25%', '50%', '75%', '85%', '95%', '100%'))\n",
    "ax.legend(labels = ['avg','f1-score', 'accuracy'])\n",
    "ax.set_title(\"Performance by Percentage of Max Features\")\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_xlabel('Percentage of Max Features')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Attempt at classifier is Naive Bays \n",
    "#nbm = naive_bayes.MultinomialNB()\n",
    "#nbm.fit(train_x_Tfidf, train_y)\n",
    "\n",
    "#Bernoulli Naive Bayes\n",
    "nbb = naive_bayes.BernoulliNB()\n",
    "nbb.fit(train_x_Tfidf, train_y)\n",
    "\n",
    "# predict the target on validation data\n",
    "#pred_nbm = nbm.predict(test_x_Tfidf)\n",
    "pred_nbb = nbb.predict(test_x_Tfidf)\n",
    "\n",
    "# output accuracy just to show it works\n",
    "print(\"NB Multinomial: Accuracy: \", accuracy_score(pred_nbm, test_y))\n",
    "print(\"NB Bernoulli: Accuracy: \", accuracy_score(pred_nbb, test_y))\n",
    "\n",
    "#svm = SVC(probability=True)\n",
    "#svm.fit(train_x_Tfidf, train_y)\n",
    "#pred_svm = svm.predict(test_x_Tfidf)\n",
    "#print(\"SVM: Accuracy: \", accuracy_score(pred_svm, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis\n",
    "\n",
    "#Predict Probability\n",
    "#nbm_probs = nbm.predict_proba(test_x_Tfidf)\n",
    "#nbm_probs = nbm_probs[:,1]\n",
    "nbb_probs = nbb.predict_proba(test_x_Tfidf)\n",
    "nbb_probs = nbb_probs[:,1]\n",
    "#svm_probs = svm.predict_proba(test_x_Tfidf)\n",
    "#svm_probs = svm_probs[:,1]\n",
    "\n",
    "#Classification Report\n",
    "#print(classification_report(test_y, pred_nbm, labels=[0,1]))\n",
    "print(classification_report(test_y, pred_nbb, labels=[0,1]))\n",
    "#print(classification_report(test_y, pred_svm, labels=[0,1]))\n",
    "\n",
    "#Calculate precision-recall\n",
    "#precision_nbm, recall_nbm, thresholds_nbm = precision_recall_curve(test_y, nbm_probs)\n",
    "precision_nbb, recall_nbb, thresholds_nbb = precision_recall_curve(test_y, nbb_probs)\n",
    "#precision_svm, recall_svm, thresholds_svm = precision_recall_curve(test_y, svm_probs)\n",
    "\n",
    "#Calculate F1\n",
    "#f1_nbm = f1_score(test_y, pred_nbm)\n",
    "f1_nbb = f1_score(test_y, pred_nbb)\n",
    "#f1_svm = f1_score(test_y, pred_svm)\n",
    "\n",
    "#Calculate precision recal auc\n",
    "# auc_nb = auc(recall_nb, precision_nb)\n",
    "# auc_svm = auc(recall_svm, precision_svm)\n",
    "\n",
    "# summarize scores\n",
    "print(\"Test Split: \", test_s)\n",
    "print(\"Max Features: \", max_f)\n",
    "#print('Naive-Bayes-Multinomial: f1=%.3f' % (f1_nbm))\n",
    "#print(\"Accuracy: \", accuracy_score(pred_nbm, test_y))\n",
    "print('Naive-Bayes-Bernoulli: f1=%.3f' % (f1_nbb))\n",
    "print(\"Accuracy: \", accuracy_score(pred_nbb, test_y))\n",
    "#print('Support Vector Machine: f1=%.3f' % (f1_svm))\n",
    "#print(\"Accuracy: \", accuracy_score(pred_svm, test_y))\n",
    "\n",
    "\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(test_y[test_y==1]) / len(test_y)\n",
    "#pyplot.plot(recall_nbm, precision_nbm, marker='.', label='Naive-Bayes Multinomial')\n",
    "pyplot.plot(recall_nbb, precision_nbb, marker='.', label='Naive-Bayes Bernoulli')\n",
    "#pyplot.plot(recall_svm, precision_svm, marker='.', label='SVM')\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning curve\n",
    "#svm_train_sizes, svm_train_scores, svm_valid_scores = learning_curve(SVC(probability=True), train_x_Tfidf, train_y,  cv=5)\n",
    "#nbm_train_sizes, nbm_train_scores, nbm_valid_scores = learning_curve(naive_bayes.MultinomialNB(), train_x_Tfidf, train_y,  cv=5)\n",
    "nbb_train_sizes, nbb_train_scores, nbb_valid_scores = learning_curve(naive_bayes.BernoulliNB(), train_x_Tfidf, train_y,  cv=5)\n",
    "\n",
    "#nbm_valid_scores = np.mean(nbm_valid_scores, axis=1)\n",
    "#nbm_train_scores = np.mean(nbm_train_scores, axis=1)\n",
    "nbb_valid_scores = np.mean(nbb_valid_scores, axis=1)\n",
    "nbb_train_scores = np.mean(nbb_train_scores, axis=1)\n",
    "#svm_valid_scores = np.mean(svm_valid_scores, axis=1)\n",
    "#svm_train_scores = np.mean(svm_train_scores, axis=1)\n",
    "\n",
    "#pyplot.plot(nbm_train_sizes, nbm_train_scores, marker='.', label='Naive-Bayes Multinomial - Training Score')\n",
    "pyplot.plot(nbb_train_sizes, nbb_train_scores, marker='.', label='Naive-Bayes Bernoulli - Training Score')\n",
    "#pyplot.plot(svm_train_sizes, svm_train_scores, marker='.', label='SVM - Training Score')\n",
    "\n",
    "pyplot.xlabel('Training Examples')\n",
    "pyplot.ylabel('Score')\n",
    "pyplot.legend()\n",
    "\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "\n",
    "#pyplot.plot(nbm_train_sizes, nbm_valid_scores, marker='.', label='Naive-Bayes Multinomial - Cross Validation Score')\n",
    "pyplot.plot(nbb_train_sizes, nbb_valid_scores, marker='.', label='Naive-Bayes Bernoulli - Cross Validation Score')\n",
    "#pyplot.plot(svm_train_sizes, svm_valid_scores, marker='.', label='SVM - Cross Validation Score')\n",
    "pyplot.xlabel('Training Examples')\n",
    "pyplot.ylabel('Score')\n",
    "pyplot.legend()\n",
    "\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x[1])\n",
    "print(type(train_x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier must be fit on training data before method is run\n",
    "def classifyPhrase(phrase, classifier, Tfidf_vect):\n",
    "    # convert all text to lowercase\n",
    "    phrase = phrase.lower()\n",
    "    # tokenize each review\n",
    "    phrase = word_tokenize(phrase)\n",
    "    \n",
    "    # unaltered words -> removes all words that arnt english, and removes stop words\n",
    "    Final_words = []\n",
    "    for word, tag in pos_tag(phrase):\n",
    "        if word not in stopwords.words('english') and word.isalpha() and wordnet.synsets(word):\n",
    "            word_Final = word  #This is where a stemmer would go\n",
    "            Final_words.append(word_Final)\n",
    "        #reviews.loc[index, 'unaltered_words'] = str(Final_words) \n",
    "        phrase = str(Final_words)\n",
    "        \n",
    "    print(phrase)\n",
    "    \n",
    "    phrase = Tfidf_vect.transform([phrase])\n",
    "        \n",
    "    \n",
    "#     #nb.fit(train_x, train_y)\n",
    "    outcome = classifier.predict(phrase)\n",
    "    print(outcome)\n",
    "    \n",
    "    if outcome == 1:\n",
    "        return \"Positive Review\"\n",
    "    else:\n",
    "        return \"Negative Review\"\n",
    "\n",
    "    \n",
    "review = classifyPhrase(\"The Movie Was Incredible and I Highly Reccomend It to a Friend\", nb, Tfidf_vect)\n",
    "print(review)\n",
    "\n",
    "review = classifyPhrase(\"The Movie Was Terrible and I hated it\", nb, Tfidf_vect)\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
