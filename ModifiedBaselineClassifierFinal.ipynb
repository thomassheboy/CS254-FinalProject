{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve,f1_score,auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproduceable results\n",
    "np.random.seed(500)\n",
    "# Read in to Pandas DataFrame and drop the first row(which contained column names as I have assigned new names)\n",
    "reviews = pd.read_csv(r\"rt_reviews.csv\", names = ['target', 'review'], encoding = 'latin-1')\n",
    "reviews = reviews.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose number of samples to use\n",
    "reviews = reviews.iloc[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize target data types to numeric\n",
    "for i in range(reviews.shape[0]):\n",
    "    if(reviews['target'].values[i] == '0'):\n",
    "        reviews['target'].values[i] = 0\n",
    "    elif(reviews['target'].values[i] == '1'):\n",
    "        reviews['target'].values[i] = 1\n",
    "for i in range(reviews.shape[0]):\n",
    "    if(isinstance(reviews['target'].values[i],str)):\n",
    "        print(\"Caught: \",reviews['target'].values[i])\n",
    "    if(reviews['target'].values[i] != 0 and reviews['target'].values[i] != 1):\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove and row where either datafield is blank, no rows contained blank data so the shape remains 480000,2 \n",
    "reviews.dropna(inplace = True)\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text to lowercase\n",
    "reviews['review'] = [entry.lower() for entry in reviews['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each review: this process converts each review into a set of words. \n",
    "reviews['review'] = [word_tokenize(entry) for entry in reviews['review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming Words: We improved upon this with Lemmatizing \n",
    "\n",
    "for index, entry in enumerate(reviews['review']):\n",
    "    index = index + 1\n",
    "    Final_words = []\n",
    "        \n",
    "    word_Stemmer = PorterStemmer()\n",
    "        \n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha() and wordnet.synsets(word):\n",
    "            word_Final = word_Stemmer.stem(word)  #This is where a stemmer would go\n",
    "            Final_words.append(word_Final)\n",
    "        reviews.loc[index, 'stemmed_words'] = str(Final_words) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing: We improved upon this with Unaltered Words\n",
    "###### Creating tags so that lemmatizer can understand verbs from nouns from adjectives \n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "for index,entry in enumerate(reviews['review']):\n",
    "    index = index+1 # Index seems to off by one, this fixes it\n",
    "    # Words that follow the rules will end up in this list\n",
    "    Final_words = []\n",
    "    \n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    \n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha() and wordnet.synsets(word):\n",
    "            word_Final = word_Lemmatized.lemmatize(word, tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "        reviews.loc[index, 'lemmatized_words'] = str(Final_words)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Unaltered Words ############\n",
    "\n",
    "for index, entry in enumerate(reviews['review']):\n",
    "    index = index + 1\n",
    "    Final_words = []\n",
    "        \n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha() and wordnet.synsets(word):\n",
    "            word_Final = word  #This is where a stemmer would go\n",
    "            Final_words.append(word_Final)\n",
    "        reviews.loc[index, 'unaltered_words'] = str(Final_words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train and Test splits \n",
    "test_s = .15\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(reviews['unaltered_words'], \n",
    "                                                                    reviews['target'], test_size=test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target data\n",
    "Encoder = LabelEncoder()\n",
    "train_y = Encoder.fit_transform(train_y)\n",
    "test_y = Encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency-Inverse Document Frequency: Improved upon this with bag of words\n",
    "max_f = 100000\n",
    "Tfidf_vect = TfidfVectorizer(stop_words='english', max_features=max_f)\n",
    "Tfidf_vect.fit_transform(reviews['unaltered_words'])\n",
    "train_x_Tfidf = Tfidf_vect.transform(train_x)\n",
    "test_x_Tfidf = Tfidf_vect.transform(test_x)\n",
    "print(Tfidf_vect.vocabulary_)\n",
    "len(Tfidf_vect.vocabulary_)\n",
    "\n",
    "\n",
    "nb = naive_bayes.MultinomialNB()\n",
    "nb.fit(train_x_Tfidf, train_y)\n",
    "\n",
    "\n",
    "pred_nb = nb.predict(test_x_Tfidf)\n",
    "\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(pred_nb, test_y))\n",
    "nb_probs = nb.predict_proba(test_x_Tfidf)\n",
    "nb_probs = nb_probs[:,1]\n",
    "print(classification_report(test_y, pred_nb, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Bag of Words #######\n",
    "max_f = 100000\n",
    "#ngram_range default is (1,1)\n",
    "#ngram_range(1,1) -> uni-gram\n",
    "#ngram_range(2,2) -> bi-gram\n",
    "#ngram_range(3,3) -> tri-gram\n",
    "#ngram_range(1,2) -> uni, bi-grams -> BEST SO FAR\n",
    "Count_vect = CountVectorizer(stop_words='english', max_features=max_f, ngram_range=(1,2))\n",
    "Count_vect.fit_transform(reviews['unaltered_words'])\n",
    "train_x_Count = Count_vect.transform(train_x)\n",
    "test_x_Count = Count_vect.transform(test_x)\n",
    "#print(Tfidf_vect.vocabulary_)\n",
    "len(Count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Bag of Words Frequency #######\n",
    "nb = naive_bayes.MultinomialNB()\n",
    "nb.fit(train_x_Count, train_y)\n",
    "\n",
    "print(train_x_Count[0])\n",
    "\n",
    "# predict the target on validation data\n",
    "pred_nb = nb.predict(test_x_Count)\n",
    "\n",
    "# output accuracy just to show it works\n",
    "print(\"Accuracy: \", accuracy_score(pred_nb, test_y))\n",
    "nb_probs = nb.predict_proba(test_x_Count)\n",
    "nb_probs = nb_probs[:,1]\n",
    "print(classification_report(test_y, pred_nb, labels=[0,1]))\n",
    "# svm = SVC(probability=True)\n",
    "# svm.fit(train_x_Count, train_y)\n",
    "# pred_svm = svm.predict(test_x_Count)\n",
    "# print(\"Accuracy: \", accuracy_score(pred_svm, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############### Lemma vs Stem vs Unaltered 480k ###################\n",
    "\n",
    "# performed on 480k samples with non english words removed\n",
    "width = .3\n",
    "pltData = pd.DataFrame({ \n",
    "'accuracy': [.7833, .7792, .7945], \n",
    "'f1-score' : [.78, .78,.795]})\n",
    "\n",
    "pltData[['f1-score', 'accuracy']].plot(kind = 'bar', width = width)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.xlim([-width, len(pltData['accuracy'])-width])\n",
    "\n",
    "ax.set_ylim([.77, .8])\n",
    "ax.set_xticklabels(('Lemmatized', 'Stemmed', 'Unaltered'))\n",
    "ax.legend(labels = ['f1-score', 'accuracy'])\n",
    "ax.set_title(\"Performance by Word Alteration type\")\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_xlabel('Word Alteration type')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "################ Graphs of the Different grams for Bag of Words ############\n",
    "\n",
    "#performed on all 100,000 examples w/ unaltered words\n",
    "\n",
    "#looking at all grams\n",
    "width = .3\n",
    "pltData = pd.DataFrame({ \n",
    "'accuracy': [.7871, .7294, .5991, .5746, .8055, .8047, .8049], \n",
    "'f1-score' : [.79, .73, .54, .48, .81, .80, .80]})\n",
    "\n",
    "pltData[['f1-score', 'accuracy']].plot(kind = 'bar', width = width)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.xlim([-width, len(pltData['accuracy'])-width])\n",
    "\n",
    "ax.set_ylim([.45, .85])\n",
    "ax.set_xticklabels(('Uni', 'Bi', 'Tri', 'Quad', 'Uni & Bi', 'Uni, Bi & Tri', 'Uni, Bi, Tri, &Quad'))\n",
    "ax.legend(labels = ['f1-score', 'accuracy'])\n",
    "ax.set_title(\"Performance by Gram Type\")\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_xlabel('Gram type')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#looking at bag of words vs. Term frequency inverse document freqeuncy \n",
    "width = .3\n",
    "pltData = pd.DataFrame({ \n",
    "'accuracy': [.8055, .7945], \n",
    "'f1-score' : [.81, .795]})\n",
    "\n",
    "pltData[['f1-score', 'accuracy']].plot(kind = 'bar', width = width)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.xlim([-width, len(pltData['accuracy'])-width])\n",
    "\n",
    "ax.set_ylim([.75, .85])\n",
    "ax.set_xticklabels(('Bag of Words','TFIDF'))\n",
    "ax.legend(labels = ['f1-score', 'accuracy'])\n",
    "ax.set_title(\"Performance by Vectorization Technique\")\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_xlabel('Vectorization Technique')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############### Alternative Plot for minumum document frequency ###################\n",
    "\n",
    "width = .25 \n",
    "pltData = pd.DataFrame({ \n",
    "'accuracy': [.7845, .7833, .7811, .7748, .7673], \n",
    "'f1-score' : [.785, .78, .78, .775, .77], \n",
    "'avg' : [.78475, .7817, .7805, .7749, .7686]})\n",
    "\n",
    "pltData[['f1-score', 'accuracy']].plot(kind = 'bar', width = width)\n",
    "pltData['avg'].plot(color = \"tab:cyan\")\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.xlim([-width, len(pltData['accuracy'])-width])\n",
    "\n",
    "ax.set_ylim([.76, .79])\n",
    "ax.set_xticklabels(('1', '5', '10', '50', '100'))\n",
    "ax.legend(labels = ['avg','f1-score', 'accuracy'])\n",
    "ax.set_title(\"Performance by Minimum document frequency\")\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_xlabel('Minimum Document Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Plot for Max features ###################\n",
    "\n",
    "width = .25 \n",
    "pltData = pd.DataFrame({ \n",
    "'accuracy': [.7748, .7810, .7841, .7845, .7845, .7845], \n",
    "'f1-score' : [.775, .78, .78, .785, .785, .785], \n",
    "'avg' : [.7749, .7805, .7820, .78475, .78475, .78475]})\n",
    "\n",
    "pltData[['f1-score', 'accuracy']].plot(kind = 'bar', width = width)\n",
    "pltData['avg'].plot(color = \"tab:cyan\")\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.xlim([-width, len(pltData['accuracy'])-width])\n",
    "\n",
    "ax.set_ylim([.77, .79])\n",
    "ax.set_xticklabels(('25%', '50%', '75%', '85%', '95%', '100%'))\n",
    "ax.legend(labels = ['avg','f1-score', 'accuracy'])\n",
    "ax.set_title(\"Performance by Percentage of Max Features\")\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_xlabel('Percentage of Max Features')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Attempt at classifier is Naive Bays \n",
    "#nbm = naive_bayes.MultinomialNB()\n",
    "#nbm.fit(train_x_Count, train_y)\n",
    "\n",
    "#Bernoulli Naive Bayes\n",
    "nbb = naive_bayes.BernoulliNB()\n",
    "nbb.fit(train_x_Count, train_y)\n",
    "\n",
    "# predict the target on validation data\n",
    "#pred_nbm = nbm.predict(test_x_Count)\n",
    "pred_nbb = nbb.predict(test_x_Count)\n",
    "\n",
    "# output accuracy just to show it works\n",
    "#print(\"NB Multinomial: Accuracy: \", accuracy_score(pred_nbm, test_y))\n",
    "print(\"NB Bernoulli: Accuracy: \", accuracy_score(pred_nbb, test_y))\n",
    "\n",
    "#svm = SVC(probability=True)\n",
    "#svm.fit(train_x_Count, train_y)\n",
    "#pred_svm = svm.predict(test_x_Count)\n",
    "#print(\"SVM: Accuracy: \", accuracy_score(pred_svm, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis\n",
    "\n",
    "#Predict Probability\n",
    "#nbm_probs = nbm.predict_proba(test_x_Count)\n",
    "#nbm_probs = nbm_probs[:,1]\n",
    "nbb_probs = nbb.predict_proba(test_x_Count)\n",
    "nbb_probs = nbb_probs[:,1]\n",
    "#svm_probs = svm.predict_proba(test_x_Count)\n",
    "#svm_probs = svm_probs[:,1]\n",
    "\n",
    "#Classification Report\n",
    "#print(classification_report(test_y, pred_nbm, labels=[0,1]))\n",
    "print(classification_report(test_y, pred_nbb, labels=[0,1]))\n",
    "#print(classification_report(test_y, pred_svm, labels=[0,1]))\n",
    "\n",
    "#Calculate precision-recall\n",
    "#precision_nbm, recall_nbm, thresholds_nbm = precision_recall_curve(test_y, nbm_probs)\n",
    "precision_nbb, recall_nbb, thresholds_nbb = precision_recall_curve(test_y, nbb_probs)\n",
    "#precision_svm, recall_svm, thresholds_svm = precision_recall_curve(test_y, svm_probs)\n",
    "\n",
    "#Calculate F1\n",
    "#f1_nbm = f1_score(test_y, pred_nbm)\n",
    "f1_nbb = f1_score(test_y, pred_nbb)\n",
    "#f1_svm = f1_score(test_y, pred_svm)\n",
    "\n",
    "#Calculate precision recal auc\n",
    "# auc_nb = auc(recall_nb, precision_nb)\n",
    "# auc_svm = auc(recall_svm, precision_svm)\n",
    "\n",
    "# summarize scores\n",
    "print(\"Test Split: \", test_s)\n",
    "print(\"Max Features: \", max_f)\n",
    "#print('Naive-Bayes-Multinomial: f1=%.3f' % (f1_nbm))\n",
    "#print(\"Accuracy: \", accuracy_score(pred_nbm, test_y))\n",
    "print('Naive-Bayes-Bernoulli: f1=%.3f' % (f1_nbb))\n",
    "print(\"Accuracy: \", accuracy_score(pred_nbb, test_y))\n",
    "#print('Support Vector Machine: f1=%.3f' % (f1_svm))\n",
    "#print(\"Accuracy: \", accuracy_score(pred_svm, test_y))\n",
    "\n",
    "\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(test_y[test_y==1]) / len(test_y)\n",
    "#pyplot.plot(recall_nbm, precision_nbm, marker='.', label='Naive-Bayes Multinomial')\n",
    "pyplot.plot(recall_nbb, precision_nbb, marker='.', label='Naive-Bayes Bernoulli')\n",
    "#pyplot.plot(recall_svm, precision_svm, marker='.', label='SVM')\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning curve\n",
    "#svm_train_sizes, svm_train_scores, svm_valid_scores = learning_curve(SVC(probability=True), train_x_Count, train_y,  cv=5)\n",
    "#nbm_train_sizes, nbm_train_scores, nbm_valid_scores = learning_curve(naive_bayes.MultinomialNB(), train_x_Count, train_y,  cv=5)\n",
    "nbb_train_sizes, nbb_train_scores, nbb_valid_scores = learning_curve(naive_bayes.BernoulliNB(), train_x_Count, train_y,  cv=5)\n",
    "\n",
    "#nbm_valid_scores = np.mean(nbm_valid_scores, axis=1)\n",
    "#nbm_train_scores = np.mean(nbm_train_scores, axis=1)\n",
    "nbb_valid_scores = np.mean(nbb_valid_scores, axis=1)\n",
    "nbb_train_scores = np.mean(nbb_train_scores, axis=1)\n",
    "#svm_valid_scores = np.mean(svm_valid_scores, axis=1)\n",
    "#svm_train_scores = np.mean(svm_train_scores, axis=1)\n",
    "\n",
    "#pyplot.plot(nbm_train_sizes, nbm_train_scores, marker='.', label='Naive-Bayes Multinomial - Training Score')\n",
    "pyplot.plot(nbb_train_sizes, nbb_train_scores, marker='.', label='Naive-Bayes Bernoulli - Training Score')\n",
    "#pyplot.plot(svm_train_sizes, svm_train_scores, marker='.', label='SVM - Training Score')\n",
    "\n",
    "pyplot.xlabel('Training Examples')\n",
    "pyplot.ylabel('Score')\n",
    "pyplot.legend()\n",
    "\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "\n",
    "#pyplot.plot(nbm_train_sizes, nbm_valid_scores, marker='.', label='Naive-Bayes Multinomial - Cross Validation Score')\n",
    "pyplot.plot(nbb_train_sizes, nbb_valid_scores, marker='.', label='Naive-Bayes Bernoulli - Cross Validation Score')\n",
    "#pyplot.plot(svm_train_sizes, svm_valid_scores, marker='.', label='SVM - Cross Validation Score')\n",
    "pyplot.xlabel('Training Examples')\n",
    "pyplot.ylabel('Score')\n",
    "pyplot.legend()\n",
    "\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier must be fit on training data before method is run\n",
    "def classifyPhrase(phrase, classifier, Count_vect):\n",
    "    # convert all text to lowercase\n",
    "    phrase = phrase.lower()\n",
    "    # tokenize each review\n",
    "    phrase = word_tokenize(phrase)\n",
    "    \n",
    "    # unaltered words -> removes all words that arnt english, and removes stop words\n",
    "    Final_words = []\n",
    "    for word, tag in pos_tag(phrase):\n",
    "        if word not in stopwords.words('english') and word.isalpha() and wordnet.synsets(word):\n",
    "            word_Final = word  #This is where a stemmer would go\n",
    "            Final_words.append(word_Final)\n",
    "        #reviews.loc[index, 'unaltered_words'] = str(Final_words) \n",
    "        phrase = str(Final_words)\n",
    "        \n",
    "    print(phrase)\n",
    "    \n",
    "    phrase = Count_vect.transform([phrase])\n",
    "        \n",
    "    \n",
    "#     #nb.fit(train_x, train_y)\n",
    "    outcome = classifier.predict(phrase)\n",
    "    print(outcome)\n",
    "    \n",
    "    if outcome == 1:\n",
    "        return \"Positive Review\"\n",
    "    else:\n",
    "        return \"Negative Review\"\n",
    "\n",
    "    \n",
    "review = classifyPhrase(\"The Movie Was Incredible and I Highly Recommend It to a Friend.  I loved it.\", nb, Count_vect)\n",
    "print(review)\n",
    "\n",
    "review = classifyPhrase(\"The Movie Was Terrible and I hated it.  Don't watch it.\", nb, Count_vect)\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
